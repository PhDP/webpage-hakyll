---
title: A gentle introduction to statistical relational learning: maths, code, and examples
tags: machine learning, artificial intelligence, manticore
---

<p>Statistical relational learning is a branch of machine learning (A.I.)
mixing ideas from probability theory and logic. I'll write another post later
to explain the motivation and a bit of history of this fascinating branch of
study but here I want to focus on a concrete example, with detailed
maths and real code.</p>

<p>The approach to statistical relational learning explained here is called
Markov logic network (MLN), <a
href='https://homes.cs.washington.edu/~pedrod/kbmn.pdf'>discovered in 2006 by
Richardson and Domingo.</a> Their paper has a nice simple example of MLN
applied to the relationship between smoking and cancer. However, it's a bit
hard to follow unless you understand logic and probabilistic graphical models
well, so I'll go into much more details here. I'll also show an interactive
session with <a href='https://github.com/PhDP/Manticore'>Manticore</a>, a small
implementation I wrote for interacting with models.</p>

<p>A Markov logic network is simply a set of formulas written in first-order
logic, each associated with a weight. We'll use this for our examples:</p>

<table style='width: 80%'>
  <tr>
    <th>Statement</th>
    <th>Weight</th>
  </tr>
  <tr>
    <td>Smoking causes cancers</td>
    <td align='center'>1.1</td>
  </tr>
  <tr>
    <td>If two people are friends and one smokes, then so does the other</td>
    <td align='center'>1.5</td>
  </tr>
</table>

<p>Using a more formal representation for logic, we'll get:</p>

\[\forall x: Smoking(x) \Rightarrow Cancer(x), 1.5;\]
\[\forall x, y: Friend(x, y) \land Smoking(x) \Rightarrow Smoking(y), 1.1;\]

<p<a href='https://en.wikipedia.org/wiki/List_of_logic_symbols'>Wikipedia has a
  nice page on the various symbols</a>, but we'll only a few: \(\forall\) is
"for all", \(\land\) is "and", and \(\Rightarrow\) is "implies". Then we have
the weight: the higher it is, the stronger the statement. Something a bit
confusing with logic is that implication is true if the left-side is false, so
if Smokine(Elaine) is false, then the formula returns 1.</p>

<p>The grand idea here is that, in pure logic, a world is false if it violates
a single formula, but in statistical relational learning, a world is less
likely if it violates formulas, especially if it violates formula with a high
weight. is the log odds between a world where F is true and a world where F is
false. It has important advances on probabilistic approaches too: a first-order
logic formula is simple to understand and interpret, it can be manipulated by
humans in computers in ways a naked probabilistic model can't.</p>

<h2>Inference in Markov logic</h2>

<p>Where's our network? It's called a Markov logic network, but all we got
is a set of weighted first-order logic formulas. While I do suggest you spread
riots and chaos in anger, there's actually a simple explanation. A Markov
logic network is a template for Markov networks. Take the formula:</p>

\[\forall x: Smoking(x) \Rightarrow Cancer(x), 1.5;\]

<p><i>x</i> is a variable, and to get a concrete model for inference from this
template, we need to constraints the MLN with a set of constants. So in essence:</p>

\[\mbox{MLN + Constants} \rightarrow \mbox{Markov Network}\]

<p>Let's say we are interested smoking/cancer for Anna, Bob, Charlotte, Dominique,
we can apply these constants to the formula to get a set of ground formulas.
The term ground means the variables are replaced by constants, that is, concrete
objects. For example in mathematics you probably encountered first-order logic formulas like:</p>

\[\forall x: Add(x, 0) = 0\]

<p>For this formula, we can have grounding by replacing x with integers</p>

\[Add(47, 0) = 0\]
\[Add(1729, 0) = 0\]

<p>Similarly, applying the set of constants \(\{Elaine, Jerry, George\}\) to our first formula yields a set of ground formulas:</p>

\[Smoking(Elaine) \Rightarrow Cancer(Elaine), 1.5;\]
\[Smoking(Jerry) \Rightarrow Cancer(Jerry), 1.5;\]
\[Smoking(George) \Rightarrow Cancer(George), 1.5;\]

<p>We could do the same thing with the other formula, but now it has two
variables:</p>

\[Friend(Elaine, Elaine) \land Smoking(Elaine) \Rightarrow Smoking(Elaine), 1.1;\]
\[Friend(Elaine, Jerry) \land Smoking(Elaine) \Rightarrow Smoking(Jerry), 1.1;\]
\[Friend(Elaine, George) \land Smoking(Elaine) \Rightarrow Smoking(George), 1.1;\]
\[Friend(Jerry, Elaine) \land Smoking(Jerry) \Rightarrow Smoking(Elaine), 1.1;\]
\[Friend(Jerry, Jerry) \land Smoking(Jerry) \Rightarrow Smoking(Jerry), 1.1;\]
\[Friend(Jerry, George) \land Smoking(Jerry) \Rightarrow Smoking(George), 1.1;\]
\[Friend(George, Elaine) \land Smoking(George) \Rightarrow Smoking(Elaine), 1.1;\]
\[Friend(George, Jerry) \land Smoking(George) \Rightarrow Smoking(Jerry), 1.1;\]
\[Friend(George, George) \land Smoking(George) \Rightarrow Smoking(George), 1.1;\]

<p>From the groundings of the two formulas, we get a set of predicates:</p>

\[\{Smoking(Elaine), Smoking(Jerry), Smoking(George), Cancer(Elaine),\]

\[Cancer(Jerry), Cancer(George), Friend(Elaine, Elaine), Friend(Elaine, Jerry),\]

\[Friend(Elaine, George), Friend(Jerry, Elaine), Friend(Jerry, Jerry),\]

\[Friend(Jerry, George), Friend(George, Elaine),\]

\[Friend(George, Jerry), Friend(Jerry, George)\}\]

<p>We now have the basic blocs for inference. Of course, we could generate a
completely different set of ground formulas and ground predicates if we
applied, say, the constants \(\{William, Anastasia, Kara, Saul, Karl, Tory,
Felix, Laura\}\), or any number of objects we're interested in. Also, we
don't need to have full data on all predicates, I'll show examples where we
have no information at all.</p>

<h2>From predicates and formulas to Markov network</h2>

<p>Hey, we still don't have our network! To finally get our network, we'll
create one node for each ground predicate, and link all nodes that are in the
same formula. We our example with the constants \(\{Elaine, jerry, George\}\),
we get the following Markov network:</p>

<div class="imagecenter">
  <img src="../images/ground_seinfeld.png" alt="ground network"/>
</div>

<p>For example, since we have</p>

\[Friend(Elaine, Elaine) \land Smoking(Elaine) \Rightarrow Smoking(Elaine), 1.1;\]

<p>These nodes are linked in the network:</p>

<div class="imagecenter">
  <img src="../images/ground_seinfeld_hl.png" alt="ground network hl"/>
</div>

<p>That said, I don't think the network is that important to understand how
Markov logic networks work. I'll just get the equation for computing
probabilities and we'll look at a detailed example:</p>

\[P(X = x) = \frac{1}{Z}\exp\left(\sum_i w_ig_i(x) \right)\]

<p>Where Z is a normalizing factor (common to all Markov networks), w_i
is the weight of the ground formula, and g_i(x) is either 0 if the formula
is false, or 1 if it's true. For example, let's take the ground formula:</p>

\[Smoking(Elaine) \Rightarrow Cancer(Elaine), 1.5;\]

<p>If in our assignment \(Smoking(Elaine)\) is true and \(Cancer(Elaine)\) is
true, the entire formula is true, thus w = 1.5 and g = 1. To understand how we
get from a network to a probability, it's best to just look at the list of
ground formulas generated.  Imagine we have an assignment where Elaine Jerry,
and Goerge are all friends with eatch, none of them smoke, and none of them has
cancer. In short, all predicates are false except: Friend(Jerry, Elaine),
Friend(Elaine, Jerry), Friend(George, Jerry), Friend(Jerry, Goerge),
Friend(Elaine, George), Friend(George, Elaine). What is the joint probability
of this assignment?</p>

<p>If you're not used to Markov networks, it can be a bit confusing.</p>

\[P(Cancer(Jerry))\]

\[P(Cancer(Jerry)) = \frac{P(Cancer(Jerry), Smoking(Elaine))}{Smoking(Elaine)}\]

<p>It can be tricky to compute <i>Z</i> since it involves all possible
assignments, but there's a shortcut specific to Markov logic networks.  , but
there's a huge shortcut to compute <i>Z</i>:</p>

\[Z = \sum_{f \in L} w_ft_fc^{n_f},\]

<p>with \(w_f\) being the weight of formula \(f\), \(t_f\) being the number of
true valuations of \(f\) (that is: ), and \(n_f\) is the number of groundings.
For some reason I've never seen this equation (probably because nobody is doing
exact inference.</p>

<h2>Code</h2>

<div class='terminal'><pre>
$ git clone https://github.com/PhDP/Manticore.git
$ cd Manticore
$ cabal install
$ cabal repl
</pre></div>

<p>Right now, it performs only exact inference, which is useful for tests...
but it does not scale (the Markov networks generated by Markov logic are
humongous). It'll be enough for exploring a model.</p>

<p>First, import the Markov logic network module:</p>

<pre><code class="haskell">ghci> import qualified Manticore.MarkovLogic as ML</code></pre>

<p>The most straighforward way to build a Markov logic network is with
<i>fromStrings</i>. This function takes an array of strings, each of which must
be a valid first-order logic formula followed (or preceded) by a number (the
weight of the formula). In this case we have:</p>

<pre><code class="haskell">ghci> let mln = ML.fromStrings ["∀x Smoking(x) ⇒ Cancer(x) 1.5", "∀x∀y Friend(x, y) ∧ Smoking(x) ⇒ Smoking(y) 1.1"]</code></pre>

<p>The strings were copy-pasted from Richardson and Domingos' paper, but the
parsers is flexible and will accept a keyboard-friendly form too:</p>

<pre><code class="haskell">ghci> ML.fromStrings ["1.5 forall x Smoking(x) implies Cancer(x)", "1.1 forall x, y Friend(x, y) and Smoking(x) implies Smoking(y)"]</code></pre>

<p>We can use <i>fmtMLN</i> function to print the Markov logic
network:</p>

<pre><code class="haskell">ghci> putStrLn (fmtMLN mln)
1.5                     ∀x Smoking(x) ⇒ Cancer(x)
1.1                     ∀x ∀y Friend(x, y) ∧ Smoking(x) ⇒ Smoking(y)</code></pre>

<p>A Markov logic network is a template for Markov networks. To get a Markov
network, we need to apply a set of constant to the Markov logic networks
Here, we will create only two constants:</p>

<pre><code class="haskell">ghci> let cs = ["Elaine", "George", "Jerry"]</code></pre>

<p>Then, we can query the network, say, what is the probability that Jerry has
cancer?</p>

<pre><code class="haskell">ghci> ML.ask cs mln "P(Cancer(Jerry))"
Just 0.6133819604540808</code></pre>

<p>The function <i>ask</i> takes a Markov logic network, a list
of terms (represented as a list of strings), and a string query. It will return
Just P, with P being a probability in the [0.0, 1.0] range, or Nothing if the
parser fails to read the query. To make the process a bit easier we'll create
a function ask that have already the mln and terms supplied:</p>

<pre><code class="haskell">ghci> let ask = MLN.ask mln cs</code></pre>

Then we can ask queries with this function:

<pre><code class="haskell">ghci> ask "P(Cancer(Jerry) and Cancer(George))"
Just 0.38061259085226223</code></pre>

<pre><code class="haskell">ghci> ask "P(Cancer(Jerry) | Smoking(George))"
Just 0.6519697695221907</code></pre>

<pre><code class="haskell">ghci> ask "P(Cancer(Jerry) | Smoking(George), Friend(George, Jerry))"
Just 0.7056438194691147</code></pre>

We could write \"Cancer(Jerry) = True\", but the parser assumes all predicates
are true if they are not assigned to a value. If we can to say something is
false, say: Jerry is not smoking, we could write \"Smoking(Jerry) = False\" or
the shorter (and preferred) \"!Smoking(Jerry)\"

<pre><code class="haskell">ghci> ask "P(Smoking(Jerry) | !Smoking(Elaine), Smoking(George), Friend(Elaine, Jerry))"
Just 0.49999999999999994

ghci> ask "P(Cancer(Anna) | !Smoking(Anna), !Smoking(Bob), !Friend(Bob, Anna))"
Just 0.5000000000000002</code></pre>

<p>We can add a logic formula to the network with the 'Manticore.MarkovLogic.tell'
function. Note that 'Manticore.MarkovLogic.tell' takes a string (just like
'Manticore.MarkovLogic.fromStrings' takes a list of strings) and will return
a new Markov logic network with the formula added. Let's say we want to add
a rule that /friends of friends are friends/, we could add this rule with a
weight of 1.0 with:</p>

<pre><code class="haskell">ghci> let mln' = tell "1.0 A.x,y,z Friend(x, y) and Friend(y, z) => Friend(x, z)" mln
ghci> putStrLn (fmtMLN mln')
1.5                     ∀x Smoking(x) ⇒ Cancer(x)
1.0                     ∀x ∀y ∀z Friend(x, y) ∧ Friend(y, z) ⇒ Friend(x, z)
1.1                     ∀x ∀y Friend(x, y) ∧ Smoking(x) ⇒ Smoking(y)</code></pre>

This time we'll ask queries with this network using four objects: Anna, Bob,
Charlotte, Dominique, again creating a shortcut function ask2 to avoid always
providing the same arguments:

<pre><code class="haskell">ghci> let ask' = ML.ask cs mln'</code></pre>

<pre><code class="haskell">ghci> ask2 "P(Cancer(Anna))"
Just 0.6093306813995714
ghci> ask2 "P(Cancer(Anna) | Smoking(Bob) and Friend(Bob, Charlotte))"
Just 0.6504904438065744
ghci> ask2 "P(Cancer(Anna) | Smoking(Bob))"
Just 0.6399247839775043
ghci> ask2 "P(Cancer(Anna) | Smoking(Bob) and Friend(Bob, Charlotte) and Friend(Charlotte, Anna))"
Just 0.7070401384366385
ghci> ask2 "P(Cancer(Anna) | Smoking(Bob) and Friend(Bob, Charlotte) and !Friend(Charlotte, Anna))"
Just 0.6329610969479669
ghci> ask2 "P(Cancer(Anna) | Smoking(Bob) and Friend(Bob, Charlotte) and Friend(Charlotte, Anna) and !Smoking(Anna))"
Just 0.4999999999999977
ghci> ask2 "P(Cancer(Anna) | !Smoking(Anna))"
Just 0.49999999999999856</code></pre>

<p>Now, something is bugging me with this model:</p>

<pre><code class="haskell">ghci> ask' "P(Cancer(Jerry) | Smoking(George))"
ghci> ask' "P(Cancer(Jerry) | Smoking(George), Friend(George, Jerry))"
ghci> ask' "P(Cancer(Jerry) | Smoking(George), Friend(Jerry, George))"
</code></pre>

<p>The problem is that friendship is (mostly) symmetricial. If we know George is friend
with Jerry, there's a very good chance that Jerry is friend with George (and thus influeced
by his smoking habit). The nice thing with Markov logic is that all formulas are connected,
so we can fix this issue by adding a formula, our new Markov logic network is:</p>

\[\forall x: Smoking(x) \Rightarrow Cancer(x), 1.5;\]
\[\forall x, y: Friend(x, y) \land Smoking(x) \Rightarrow Smoking(y), 1.1;\]
\[\forall x, y: Friend(x, y) \iff Friend(y, x), 2.0;\]

<p>The last formula has the \(\iff\) operator, which is true either when both
sides are true, or when both sides are false. Thus, this formula says: if \(x\)
is friend with \(y\), then \(y\) is friend with \(x\), and if \(x\) is not friend
with \(y\), then \(y\) is not friend with \(x\). Since it's probabilistic, it doesn't
need to be true all the time, but I give it a fairly high weight. And now we have:</p>

<pre><code class="haskell">ghci> let mln'' = ML.tell "ForAll x, y: Friend(x, y) iff Friend(y, z) 2.0" mln
ghci> ML.fmtMLN mln''
...
ghci> let ask'' = ML.ask cs mln''
ghci> ask'' "P(Cancer(Jerry) | Smoking(George))"
Just xxx
ghci> ask'' "P(Cancer(Jerry) | Smoking(George), Friend(George, Jerry))"
Just xxx
ghci> ask'' "P(Cancer(Jerry) | Smoking(George), Friend(Jerry, George))"
Just xxx
</code></pre>

<p>We ask the exact same query, but with the knowledge that friendship is (mostly)
symmetrical, we get different probabilities.</p>

