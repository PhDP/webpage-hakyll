---
title: Structure learning in statistical relational learning
tags: machine learning, artificial intelligence, markov logic
---

<p>In a previous post I described Markov logic networks, a way to unify
statistical and logic approaches to machine learning. To (very briefly) recap,
a Markov logic network is a set of first-order logic formulas, each with a
weight, e.g.</p>

\[dctOrder(t, R1) ∧ relE2T(e, t, R2) ⇒ relDCT(e, R3), 1.1;\]
\[dctOrder(t, R1) ∧ relDCT(e, R2) ⇒ relE2T(e, t, R3), 1.5;\]
\[elDCT(e1, R1) ∧ relDCT(e2, R2) ⇒ relE2E(e1, e2, R3), 1.9;\]
\[relDCT(e1, R1) ∧ relE2E(e1, e2, R2) ⇒ relDCT(e2, R3), 0.8;\]
\[relE2T(e1, t1, R1) ∧ relT2T(t1, t2, R2) ∧ relE2T(e2, t2, R3) ⇒ relE2E(e1, e2, R4), 1.2;\]
\[relE2T(e2, t2, R1) ∧ relT2T(t1, t2, R2) ∧ relE2E(e1, e2, R3) ⇒ relE2T(e1, t1, R4), 1.3.\]

<p>This MLN is from a paper by <a
  href='http://www.aclweb.org/anthology/P09-1046.pdf'>Yoshikawa et al.
  (2009)</a>, where the authors manually designed the logical formulas, used
machine learning to establish the weights. Their approaches was able to beat
both pure logic-based approaches and pure-machine learning approaches. It's an
interesting combo since learning formulas is difficult for computers as a good
understanding of the context is very useful, and weight learning is difficult
for humans because of the precise computations involved. That said, this case
if not necessarily representative: we understand very well how to (in part
because it's all about natural language). Moreover, the problem is simple
enough to be described effectively in only six logical formulas. Not all
problems are that simple.</p>

<p>In a nutshell, there are three strategies to discover the structure (that
is: find the logical formulas):</p>

<ol style='list-style-type: upper-roman'>
  <li><b>Human expert</b>: We can gently ask a human expert to write down formulas.</li>
  <li><b>Transfer learning</b> We can attempt to use an existent structure from a similar problem to
    transfer knowledge, e.g. <a href='http://www.cs.utexas.edu/users/ml/papers/mihalkova-aaai07.pdf'>Lilyana Mihalkova's TAMAR algorithm</a>.</li>
  <li><b>Tabula rasa</b>: there's basically no name for this approach, perhaps because it's the de facto
  standard. Here, we use an algorithm to learn the structure from data without any prior information nor
  the help of these pesky human experts.</li>
</ol>

<p>The correct approach depends largely on two factors: how much data do we have, and how
complex the structure is. Complex structures are unlikely to be found by experts, this
is basically why we have machine learning in the first place. For these problems, transfer
is necessary when data is sparse, but we can hope to succeed with a <i>tabula rasa</i> approach
with plentiful of data.</p>

<div class="imagecenter">
  <img src="../images/structure-learning-strategies.png" alt="Structure Learning Strategies"/>
</div>

<p>There are also many middle-road approaches. For example, we know algorithms
to revise existing structures, a technique extensively used for transfer
learning. It could be used to establish a theory by an expert and then attempt
to revise it to find better alternatives.</p>

